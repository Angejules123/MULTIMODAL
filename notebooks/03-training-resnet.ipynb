{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4080f3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Master data science\\MPDS3_2025\\projet federal\\projet\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Projet: e:\\Master data science\\MPDS3_2025\\projet federal\\projet\n",
      "üî• PyTorch: 2.9.0+cpu\n",
      "üíª Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Configuration de l'entra√Ænement - IMPORTS ET SETUP\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import models, transforms\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# M√©triques\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    ")\n",
    "\n",
    "# Configuration plots\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Paths\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"üìÇ Projet: {project_root}\")\n",
    "print(f\"üî• PyTorch: {torch.__version__}\")\n",
    "print(f\"üíª Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   M√©moire: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0a38485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öôÔ∏è  CONFIGURATION:\n",
      "{\n",
      "  \"IMAGE_SIZE\": \"(400, 400)\",\n",
      "  \"BATCH_SIZE\": \"16\",\n",
      "  \"NUM_WORKERS\": \"4\",\n",
      "  \"CLASSES\": \"['NonDemented', 'VeryMildDemented', 'MildDemented', 'ModerateDemented']\",\n",
      "  \"NUM_CLASSES\": \"4\",\n",
      "  \"MODEL_NAME\": \"resnet152\",\n",
      "  \"PRETRAINED\": \"True\",\n",
      "  \"FREEZE_BACKBONE\": \"False\",\n",
      "  \"EPOCHS\": \"50\",\n",
      "  \"LEARNING_RATE\": \"0.0001\",\n",
      "  \"WEIGHT_DECAY\": \"0.0001\",\n",
      "  \"MOMENTUM\": \"0.9\",\n",
      "  \"OPTIMIZER\": \"Adam\",\n",
      "  \"SCHEDULER\": \"ReduceLROnPlateau\",\n",
      "  \"SCHEDULER_PATIENCE\": \"5\",\n",
      "  \"SCHEDULER_FACTOR\": \"0.1\",\n",
      "  \"EARLY_STOPPING\": \"True\",\n",
      "  \"PATIENCE\": \"10\",\n",
      "  \"SEED\": \"42\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Configuration globale\n",
    "CONFIG = {\n",
    "    # Donn√©es\n",
    "    'DATA_PATH': project_root / 'data' / 'augmented',\n",
    "    'IMAGE_SIZE': (400, 400),\n",
    "    'BATCH_SIZE': 16,  # Ajuster selon GPU\n",
    "    'NUM_WORKERS': 4,\n",
    "    \n",
    "    # Classes\n",
    "    'CLASSES': ['NonDemented', 'VeryMildDemented', 'MildDemented', 'ModerateDemented'],\n",
    "    'NUM_CLASSES': 4,\n",
    "    \n",
    "    # Mod√®le\n",
    "    'MODEL_NAME': 'resnet152',  # ou 'resnet101'\n",
    "    'PRETRAINED': True,\n",
    "    'FREEZE_BACKBONE': False,  # Fine-tuning complet\n",
    "    \n",
    "    # Entra√Ænement\n",
    "    'EPOCHS': 50,\n",
    "    'LEARNING_RATE': 0.0001,\n",
    "    'WEIGHT_DECAY': 1e-4,\n",
    "    'MOMENTUM': 0.9,\n",
    "    \n",
    "    # Optimisation\n",
    "    'OPTIMIZER': 'Adam',  # 'Adam' ou 'SGD'\n",
    "    'SCHEDULER': 'ReduceLROnPlateau',\n",
    "    'SCHEDULER_PATIENCE': 5,\n",
    "    'SCHEDULER_FACTOR': 0.1,\n",
    "    \n",
    "    # Early Stopping\n",
    "    'EARLY_STOPPING': True,\n",
    "    'PATIENCE': 10,\n",
    "    \n",
    "    # Sauvegarde\n",
    "    'SAVE_DIR': project_root / 'models',\n",
    "    'LOG_DIR': project_root / 'logs',\n",
    "    'FIG_DIR': project_root / 'figures',\n",
    "    \n",
    "    # Seed\n",
    "    'SEED': 42,\n",
    "}\n",
    "\n",
    "# Cr√©er dossiers\n",
    "CONFIG['SAVE_DIR'].mkdir(exist_ok=True)\n",
    "CONFIG['LOG_DIR'].mkdir(exist_ok=True)\n",
    "CONFIG['FIG_DIR'].mkdir(exist_ok=True)\n",
    "\n",
    "# Reproductibilit√©\n",
    "torch.manual_seed(CONFIG['SEED'])\n",
    "np.random.seed(CONFIG['SEED'])\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(CONFIG['SEED'])\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(\"\\n‚öôÔ∏è  CONFIGURATION:\")\n",
    "print(json.dumps({k: str(v) for k, v in CONFIG.items() if k not in ['SAVE_DIR', 'LOG_DIR', 'FIG_DIR', 'DATA_PATH']}, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c03c400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ CR√âATION DES DATASETS\n",
      "============================================================\n",
      "  Loaded 32391 samples from train\n",
      "  Loaded 1994 samples from val\n",
      "  Loaded 3086 samples from test\n",
      "\n",
      "‚úÖ Train: 32391 images, 2025 batches\n",
      "‚úÖ Val:   1994 images, 125 batches\n",
      "‚úÖ Test:  3086 images, 193 batches\n"
     ]
    }
   ],
   "source": [
    "# Dataset et DataLoaders\n",
    "class BrainMRIDataset(Dataset):\n",
    "    \"\"\"Dataset pour IRM c√©r√©brales avec augmentation\"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir, transform=None, classes=None):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transform = transform\n",
    "        self.classes = classes or CONFIG['CLASSES']\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "        \n",
    "        # Charger images\n",
    "        self.samples = []\n",
    "        for class_name in self.classes:\n",
    "            class_path = self.root_dir / class_name\n",
    "            if class_path.exists():\n",
    "                for img_path in class_path.glob('*.jpg'):\n",
    "                    self.samples.append((str(img_path), self.class_to_idx[class_name]))\n",
    "        \n",
    "        print(f\"  Loaded {len(self.samples)} samples from {root_dir.name}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        \n",
    "        # Charger image\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Appliquer transformations\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image)\n",
    "            image = transformed['image']\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Transformations\n",
    "def get_transforms(mode='train'):\n",
    "    \"\"\"Pipeline d'augmentation\"\"\"\n",
    "    if mode == 'train':\n",
    "        return A.Compose([\n",
    "            A.Resize(CONFIG['IMAGE_SIZE'][0], CONFIG['IMAGE_SIZE'][1]),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.Rotate(limit=10, p=0.3),\n",
    "            A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=10, p=0.3),\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "            A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
    "            A.GaussianBlur(blur_limit=(3, 5), p=0.2),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    else:\n",
    "        return A.Compose([\n",
    "            A.Resize(CONFIG['IMAGE_SIZE'][0], CONFIG['IMAGE_SIZE'][1]),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "# Cr√©er datasets\n",
    "print(\"\\nüì¶ CR√âATION DES DATASETS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "train_dataset = BrainMRIDataset(\n",
    "    root_dir=CONFIG['DATA_PATH'] / 'train',\n",
    "    transform=get_transforms('train'),\n",
    "    classes=CONFIG['CLASSES']\n",
    ")\n",
    "\n",
    "val_dataset = BrainMRIDataset(\n",
    "    root_dir=CONFIG['DATA_PATH'] / 'val',\n",
    "    transform=get_transforms('val'),\n",
    "    classes=CONFIG['CLASSES']\n",
    ")\n",
    "\n",
    "test_dataset = BrainMRIDataset(\n",
    "    root_dir=CONFIG['DATA_PATH'] / 'test',\n",
    "    transform=get_transforms('test'),\n",
    "    classes=CONFIG['CLASSES']\n",
    ")\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG['BATCH_SIZE'],\n",
    "    shuffle=True,\n",
    "    num_workers=CONFIG['NUM_WORKERS'],\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=CONFIG['BATCH_SIZE'],\n",
    "    shuffle=False,\n",
    "    num_workers=CONFIG['NUM_WORKERS'],\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=CONFIG['BATCH_SIZE'],\n",
    "    shuffle=False,\n",
    "    num_workers=CONFIG['NUM_WORKERS'],\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Train: {len(train_dataset)} images, {len(train_loader)} batches\")\n",
    "print(f\"‚úÖ Val:   {len(val_dataset)} images, {len(val_loader)} batches\")\n",
    "print(f\"‚úÖ Test:  {len(test_dataset)} images, {len(test_loader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2357b55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèóÔ∏è  CR√âATION DU MOD√àLE: RESNET152\n",
      "============================================================\n",
      "‚úÖ Mod√®le charg√©: resnet152\n",
      "   Pretrained: True\n",
      "üî• Backbone d√©gel√© (fine-tuning complet)\n",
      "\n",
      "üìä Architecture de la t√™te:\n",
      "   Input: 2048 features\n",
      "   Hidden: 512 neurons (Dropout 0.5, ReLU, Dropout 0.3)\n",
      "   Output: 4 classes\n",
      "\n",
      "üìà Param√®tres:\n",
      "   Total: 59,194,948\n",
      "   Entra√Ænables: 59,194,948\n",
      "   Gel√©s: 0\n",
      "\n",
      "‚úÖ Mod√®le d√©plac√© sur: cpu\n"
     ]
    }
   ],
   "source": [
    "# Cr√©ation du mod√®le\n",
    "def create_model(model_name='resnet152', num_classes=4, pretrained=True, freeze_backbone=False):\n",
    "    \"\"\"Cr√©e et configure le mod√®le ResNet\"\"\"\n",
    "    \n",
    "    print(f\"\\nüèóÔ∏è  CR√âATION DU MOD√àLE: {model_name.upper()}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Charger mod√®le pretrained\n",
    "    if model_name == 'resnet152':\n",
    "        if pretrained:\n",
    "            weights = models.ResNet152_Weights.IMAGENET1K_V2\n",
    "            model = models.resnet152(weights=weights)\n",
    "        else:\n",
    "            model = models.resnet152(weights=None)\n",
    "    elif model_name == 'resnet101':\n",
    "        if pretrained:\n",
    "            weights = models.ResNet101_Weights.IMAGENET1K_V2\n",
    "            model = models.resnet101(weights=weights)\n",
    "        else:\n",
    "            model = models.resnet101(weights=None)\n",
    "    else:\n",
    "        raise ValueError(f\"Mod√®le {model_name} non support√©\")\n",
    "    \n",
    "    print(f\"‚úÖ Mod√®le charg√©: {model_name}\")\n",
    "    print(f\"   Pretrained: {pretrained}\")\n",
    "    \n",
    "    # Geler le backbone si demand√©\n",
    "    if freeze_backbone:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        print(f\"‚ùÑÔ∏è  Backbone gel√© (fine-tuning t√™te seulement)\")\n",
    "    else:\n",
    "        print(f\"üî• Backbone d√©gel√© (fine-tuning complet)\")\n",
    "    \n",
    "    # Modifier la derni√®re couche\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(num_features, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(512, num_classes)\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüìä Architecture de la t√™te:\")\n",
    "    print(f\"   Input: {num_features} features\")\n",
    "    print(f\"   Hidden: 512 neurons (Dropout 0.5, ReLU, Dropout 0.3)\")\n",
    "    print(f\"   Output: {num_classes} classes\")\n",
    "    \n",
    "    # Compter param√®tres\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"\\nüìà Param√®tres:\")\n",
    "    print(f\"   Total: {total_params:,}\")\n",
    "    print(f\"   Entra√Ænables: {trainable_params:,}\")\n",
    "    print(f\"   Gel√©s: {total_params - trainable_params:,}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Cr√©er mod√®le\n",
    "model = create_model(\n",
    "    model_name=CONFIG['MODEL_NAME'],\n",
    "    num_classes=CONFIG['NUM_CLASSES'],\n",
    "    pretrained=CONFIG['PRETRAINED'],\n",
    "    freeze_backbone=CONFIG['FREEZE_BACKBONE']\n",
    ")\n",
    "\n",
    "# D√©placer sur GPU\n",
    "model = model.to(device)\n",
    "print(f\"\\n‚úÖ Mod√®le d√©plac√© sur: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07085614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öôÔ∏è  CONFIGURATION D'ENTRA√éNEMENT\n",
      "============================================================\n",
      "Loss: CrossEntropyLoss\n",
      "Optimizer: Adam\n",
      "Learning Rate: 0.0001\n",
      "Weight Decay: 0.0001\n",
      "Scheduler: ReduceLROnPlateau\n",
      "Early Stopping: True (patience=10)\n"
     ]
    }
   ],
   "source": [
    "# Configuration de l'entra√Ænement - CORRIG√âE\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "if CONFIG['OPTIMIZER'] == 'Adam':\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=CONFIG['LEARNING_RATE'],\n",
    "        weight_decay=CONFIG['WEIGHT_DECAY']\n",
    "    )\n",
    "elif CONFIG['OPTIMIZER'] == 'SGD':\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=CONFIG['LEARNING_RATE'],\n",
    "        momentum=CONFIG['MOMENTUM'],\n",
    "        weight_decay=CONFIG['WEIGHT_DECAY']\n",
    "    )\n",
    "\n",
    "# Learning rate scheduler - CORRECTION: suppression du param√®tre 'verbose'\n",
    "if CONFIG['SCHEDULER'] == 'ReduceLROnPlateau':\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='min',\n",
    "        factor=CONFIG['SCHEDULER_FACTOR'],\n",
    "        patience=CONFIG['SCHEDULER_PATIENCE']\n",
    "    )\n",
    "elif CONFIG['SCHEDULER'] == 'CosineAnnealingLR':\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer,\n",
    "        T_max=CONFIG['EPOCHS'],\n",
    "        eta_min=1e-6\n",
    "    )\n",
    "\n",
    "print(\"\\n‚öôÔ∏è  CONFIGURATION D'ENTRA√éNEMENT\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Loss: CrossEntropyLoss\")\n",
    "print(f\"Optimizer: {CONFIG['OPTIMIZER']}\")\n",
    "print(f\"Learning Rate: {CONFIG['LEARNING_RATE']}\")\n",
    "print(f\"Weight Decay: {CONFIG['WEIGHT_DECAY']}\")\n",
    "print(f\"Scheduler: {CONFIG['SCHEDULER']}\")\n",
    "print(f\"Early Stopping: {CONFIG['EARLY_STOPPING']} (patience={CONFIG['PATIENCE']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36c2eb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fonctions d'entra√Ænement pr√™tes\n"
     ]
    }
   ],
   "source": [
    "# Fonctions d'entra√Ænement\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    \"\"\"Entra√Æne une epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Training')\n",
    "    for images, labels in pbar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistiques\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # Mise √† jour barre\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{100.*correct/total:.2f}%'\n",
    "        })\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100. * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate_epoch(model, loader, criterion, device):\n",
    "    \"\"\"Valide une epoch\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader, desc='Validation')\n",
    "        for images, labels in pbar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Statistiques\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            # Sauvegarder pour m√©triques\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(torch.softmax(outputs, dim=1).cpu().numpy())\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{100.*correct/total:.2f}%'\n",
    "            })\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100. * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc, all_preds, all_labels, all_probs\n",
    "\n",
    "print(\"‚úÖ Fonctions d'entra√Ænement pr√™tes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51ecc18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ D√âBUT DE L'ENTRA√éNEMENT\n",
      "============================================================\n",
      "Epochs: 50\n",
      "Batch size: 16\n",
      "Device: cpu\n",
      "============================================================\n",
      "\n",
      "üìÖ Epoch 1/50\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/2025 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# BOUCLE D'ENTRA√éNEMENT PRINCIPALE\n",
    "print(\"\\nüöÄ D√âBUT DE L'ENTRA√éNEMENT\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Epochs: {CONFIG['EPOCHS']}\")\n",
    "print(f\"Batch size: {CONFIG['BATCH_SIZE']}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Historique\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': [],\n",
    "    'lr': []\n",
    "}\n",
    "\n",
    "# Early stopping\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_model_path = CONFIG['SAVE_DIR'] / f\"{CONFIG['MODEL_NAME']}_best.pth\"\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(CONFIG['EPOCHS']):\n",
    "    print(f\"\\nüìÖ Epoch {epoch+1}/{CONFIG['EPOCHS']}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Entra√Ænement\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Validation\n",
    "    val_loss, val_acc, val_preds, val_labels, val_probs = validate_epoch(\n",
    "        model, val_loader, criterion, device\n",
    "    )\n",
    "    \n",
    "    # Scheduler\n",
    "    if CONFIG['SCHEDULER'] == 'ReduceLROnPlateau':\n",
    "        scheduler.step(val_loss)\n",
    "    else:\n",
    "        scheduler.step()\n",
    "    \n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Sauvegarder historique\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['lr'].append(current_lr)\n",
    "    \n",
    "    # Affichage des r√©sultats de l'epoch\n",
    "    print(f\"\\nüìä R√©sultats Epoch {epoch+1}:\")\n",
    "    print(f\"   Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"   Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.2f}%\")\n",
    "    print(f\"   Learning Rate: {current_lr:.6f}\")\n",
    "    \n",
    "    # Sauvegarde du meilleur mod√®le\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        \n",
    "        # Sauvegarder le mod√®le\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'val_acc': val_acc,\n",
    "            'history': history,\n",
    "            'config': CONFIG\n",
    "        }, best_model_path)\n",
    "        \n",
    "        print(f\"üíæ Meilleur mod√®le sauvegard√©! (val_loss: {val_loss:.4f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"‚è≥ Early stopping: {patience_counter}/{CONFIG['PATIENCE']}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if CONFIG['EARLY_STOPPING'] and patience_counter >= CONFIG['PATIENCE']:\n",
    "        print(f\"\\nüõë ARR√äT PR√âCOCE ACTIV√â apr√®s {epoch+1} epochs!\")\n",
    "        print(f\"   Meilleur val_loss: {best_val_loss:.4f}\")\n",
    "        break\n",
    "\n",
    "# Calcul du temps total\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "\n",
    "print(f\"\\n‚úÖ ENTRA√éNEMENT TERMIN√â en {total_time/60:.2f} minutes\")\n",
    "print(f\"   Meilleur val_loss: {best_val_loss:.4f}\")\n",
    "\n",
    "# Sauvegarde de l'historique complet\n",
    "history_file = CONFIG['LOG_DIR'] / f\"{CONFIG['MODEL_NAME']}_history.json\"\n",
    "with open(history_file, 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "\n",
    "print(f\"üíæ Historique sauvegard√©: {history_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c45997de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tous les imports sont faits!\n",
      "‚úÖ Classe BrainMRIDataset d√©finie!\n"
     ]
    }
   ],
   "source": [
    "# CELLULE 1: D√âFINITION DE LA CLASSE DATASET\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import models, transforms\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# M√©triques\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Tous les imports sont faits!\")\n",
    "\n",
    "class BrainMRIDataset(Dataset):\n",
    "    \"\"\"Dataset pour IRM c√©r√©brales avec augmentation\"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir, transform=None, classes=None):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transform = transform\n",
    "        self.classes = classes or ['NonDemented', 'VeryMildDemented', 'MildDemented', 'ModerateDemented']\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "        \n",
    "        # Charger images\n",
    "        self.samples = []\n",
    "        for class_name in self.classes:\n",
    "            class_path = self.root_dir / class_name\n",
    "            if class_path.exists():\n",
    "                for img_path in class_path.glob('*.jpg'):\n",
    "                    self.samples.append((str(img_path), self.class_to_idx[class_name]))\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è  Dossier non trouv√©: {class_path}\")\n",
    "        \n",
    "        print(f\"  ‚úÖ Charg√© {len(self.samples)} √©chantillons depuis {root_dir.name}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        \n",
    "        # Charger image\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            print(f\"‚ùå Impossible de charger: {img_path}\")\n",
    "            # Retourner une image noire en cas d'erreur\n",
    "            image = np.zeros((224, 224, 3), dtype=np.uint8)\n",
    "        else:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Appliquer transformations\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image)\n",
    "            image = transformed['image']\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "print(\"‚úÖ Classe BrainMRIDataset d√©finie!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb59d82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíª Device: cpu\n",
      "‚úÖ Configuration d√©finie!\n"
     ]
    }
   ],
   "source": [
    "# CELLULE 2: CONFIGURATION\n",
    "# Paths\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üíª Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   M√©moire: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "CONFIG = {\n",
    "    'DATA_PATH': project_root / 'data' / 'augmented',\n",
    "    'IMAGE_SIZE': (256, 256),\n",
    "    'BATCH_SIZE': 32,\n",
    "    'NUM_WORKERS': 6,\n",
    "    'CLASSES': ['NonDemented', 'VeryMildDemented', 'MildDemented', 'ModerateDemented'],\n",
    "    'NUM_CLASSES': 4,\n",
    "    'MODEL_NAME': 'resnet101',\n",
    "    'PRETRAINED': True,\n",
    "    'FREEZE_BACKBONE': False,\n",
    "    'EPOCHS': 40,\n",
    "    'LEARNING_RATE': 0.0005,\n",
    "    'WEIGHT_DECAY': 1e-4,\n",
    "    'OPTIMIZER': 'AdamW',\n",
    "    'SCHEDULER': 'CosineAnnealingLR',\n",
    "    'EARLY_STOPPING': True,\n",
    "    'PATIENCE': 8,\n",
    "    'SAVE_DIR': project_root / 'models',\n",
    "    'LOG_DIR': project_root / 'logs', \n",
    "    'FIG_DIR': project_root / 'figures',\n",
    "    'SEED': 42,\n",
    "}\n",
    "\n",
    "# Cr√©er dossiers\n",
    "CONFIG['SAVE_DIR'].mkdir(exist_ok=True)\n",
    "CONFIG['LOG_DIR'].mkdir(exist_ok=True)\n",
    "CONFIG['FIG_DIR'].mkdir(exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Configuration d√©finie!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf33dff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88a1db5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fonction create_model d√©finie!\n"
     ]
    }
   ],
   "source": [
    "# CELLULE 1: FONCTION DE CR√âATION DU MOD√àLE\n",
    "def create_model(model_name='resnet101', num_classes=4, pretrained=True, freeze_backbone=False):\n",
    "    \"\"\"Cr√©e et configure le mod√®le ResNet\"\"\"\n",
    "    \n",
    "    print(f\"üèóÔ∏è  CR√âATION DU MOD√àLE: {model_name.upper()}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Charger mod√®le pretrained\n",
    "    if model_name == 'resnet152':\n",
    "        if pretrained:\n",
    "            weights = models.ResNet152_Weights.IMAGENET1K_V2\n",
    "            model = models.resnet152(weights=weights)\n",
    "        else:\n",
    "            model = models.resnet152(weights=None)\n",
    "    elif model_name == 'resnet101':\n",
    "        if pretrained:\n",
    "            weights = models.ResNet101_Weights.IMAGENET1K_V2\n",
    "            model = models.resnet101(weights=weights)\n",
    "        else:\n",
    "            model = models.resnet101(weights=None)\n",
    "    elif model_name == 'resnet50':\n",
    "        if pretrained:\n",
    "            weights = models.ResNet50_Weights.IMAGENET1K_V2\n",
    "            model = models.resnet50(weights=weights)\n",
    "        else:\n",
    "            model = models.resnet50(weights=None)\n",
    "    else:\n",
    "        raise ValueError(f\"Mod√®le {model_name} non support√©\")\n",
    "    \n",
    "    print(f\"‚úÖ Mod√®le charg√©: {model_name}\")\n",
    "    print(f\"   Pretrained: {pretrained}\")\n",
    "    \n",
    "    # Geler le backbone si demand√©\n",
    "    if freeze_backbone:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        print(f\"‚ùÑÔ∏è  Backbone gel√© (fine-tuning t√™te seulement)\")\n",
    "    else:\n",
    "        print(f\"üî• Backbone d√©gel√© (fine-tuning complet)\")\n",
    "    \n",
    "    # Modifier la derni√®re couche\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(num_features, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(512, num_classes)\n",
    "    )\n",
    "    \n",
    "    print(f\"üìä Architecture de la t√™te:\")\n",
    "    print(f\"   Input: {num_features} features\")\n",
    "    print(f\"   Hidden: 512 neurons (Dropout 0.5, ReLU, Dropout 0.3)\")\n",
    "    print(f\"   Output: {num_classes} classes\")\n",
    "    \n",
    "    # Compter param√®tres\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"üìà Param√®tres:\")\n",
    "    print(f\"   Total: {total_params:,}\")\n",
    "    print(f\"   Entra√Ænables: {trainable_params:,}\")\n",
    "    print(f\"   Gel√©s: {total_params - trainable_params:,}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"‚úÖ Fonction create_model d√©finie!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df3758b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèóÔ∏è  CR√âATION DU MOD√àLE...\n",
      "üèóÔ∏è  CR√âATION DU MOD√àLE: RESNET101\n",
      "============================================================\n",
      "Downloading: \"https://download.pytorch.org/models/resnet101-cd907fc2.pth\" to C:\\Users\\adnan/.cache\\torch\\hub\\checkpoints\\resnet101-cd907fc2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171M/171M [02:01<00:00, 1.47MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Mod√®le charg√©: resnet101\n",
      "   Pretrained: True\n",
      "üî• Backbone d√©gel√© (fine-tuning complet)\n",
      "üìä Architecture de la t√™te:\n",
      "   Input: 2048 features\n",
      "   Hidden: 512 neurons (Dropout 0.5, ReLU, Dropout 0.3)\n",
      "   Output: 4 classes\n",
      "üìà Param√®tres:\n",
      "   Total: 43,551,300\n",
      "   Entra√Ænables: 43,551,300\n",
      "   Gel√©s: 0\n",
      "‚öôÔ∏è  CONFIGURATION DE L'OPTIMIZER...\n",
      "‚úÖ Configuration termin√©e!\n"
     ]
    }
   ],
   "source": [
    "# 1. CONFIGURATION OPTIMIS√âE\n",
    "CONFIG = {\n",
    "    'DATA_PATH': project_root / 'data' / 'augmented',\n",
    "    'IMAGE_SIZE': (256, 256),\n",
    "    'BATCH_SIZE': 32,\n",
    "    'NUM_WORKERS': 6,\n",
    "    'CLASSES': ['NonDemented', 'VeryMildDemented', 'MildDemented', 'ModerateDemented'],\n",
    "    'NUM_CLASSES': 4,\n",
    "    'MODEL_NAME': 'resnet101',\n",
    "    'PRETRAINED': True,\n",
    "    'FREEZE_BACKBONE': False,\n",
    "    'EPOCHS': 40,\n",
    "    'LEARNING_RATE': 0.0005,\n",
    "    'WEIGHT_DECAY': 1e-4,\n",
    "    'OPTIMIZER': 'AdamW',\n",
    "    'SCHEDULER': 'CosineAnnealingLR',\n",
    "    'EARLY_STOPPING': True,\n",
    "    'PATIENCE': 8,\n",
    "    'SAVE_DIR': project_root / 'models',\n",
    "    'LOG_DIR': project_root / 'logs', \n",
    "    'FIG_DIR': project_root / 'figures',\n",
    "    'SEED': 42,\n",
    "}\n",
    "\n",
    "# 2. CR√âATION DU MOD√àLE D'ABORD\n",
    "print(\"üèóÔ∏è  CR√âATION DU MOD√àLE...\")\n",
    "model = create_model(\n",
    "    model_name=CONFIG['MODEL_NAME'],\n",
    "    num_classes=CONFIG['NUM_CLASSES'],\n",
    "    pretrained=CONFIG['PRETRAINED'],\n",
    "    freeze_backbone=CONFIG['FREEZE_BACKBONE']\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# 3. CONFIGURATION OPTIMIZER APR√àS\n",
    "print(\"‚öôÔ∏è  CONFIGURATION DE L'OPTIMIZER...\")\n",
    "\n",
    "# Mixed Precision Scaler\n",
    "scaler = GradScaler() if torch.cuda.is_available() else None\n",
    "\n",
    "# Optimizer\n",
    "if CONFIG['OPTIMIZER'] == 'AdamW':\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=CONFIG['LEARNING_RATE'],\n",
    "        weight_decay=CONFIG['WEIGHT_DECAY'],\n",
    "        betas=(0.9, 0.999)\n",
    "    )\n",
    "else:\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=CONFIG['LEARNING_RATE'],\n",
    "        weight_decay=CONFIG['WEIGHT_DECAY']\n",
    "    )\n",
    "\n",
    "# Scheduler\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, \n",
    "    T_max=CONFIG['EPOCHS'],\n",
    "    eta_min=1e-6\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"‚úÖ Configuration termin√©e!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88a86f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ CR√âATION DES DATASETS ET DATALOADERS...\n",
      "  ‚úÖ Charg√© 32391 √©chantillons depuis train\n",
      "  ‚úÖ Charg√© 1994 √©chantillons depuis val\n",
      "  ‚úÖ Charg√© 3086 √©chantillons depuis test\n",
      "‚úÖ Train: 32391 images, 1013 batches\n",
      "‚úÖ Val:   1994 images, 63 batches\n",
      "‚úÖ Test:  3086 images, 97 batches\n"
     ]
    }
   ],
   "source": [
    "# CELLULE 4: CR√âATION DES DATALOADERS\n",
    "print(\"üì¶ CR√âATION DES DATASETS ET DATALOADERS...\")\n",
    "\n",
    "# Transformations optimis√©es\n",
    "def get_transforms_optimized(mode='train'):\n",
    "    \"\"\"Pipeline d'augmentation optimis√©\"\"\"\n",
    "    if mode == 'train':\n",
    "        return A.Compose([\n",
    "            A.Resize(CONFIG['IMAGE_SIZE'][0], CONFIG['IMAGE_SIZE'][1]),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.Rotate(limit=10, p=0.3),\n",
    "            A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=10, p=0.3),\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "            A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    else:\n",
    "        return A.Compose([\n",
    "            A.Resize(CONFIG['IMAGE_SIZE'][0], CONFIG['IMAGE_SIZE'][1]),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "# Cr√©er datasets\n",
    "train_dataset = BrainMRIDataset(\n",
    "    root_dir=CONFIG['DATA_PATH'] / 'train',\n",
    "    transform=get_transforms_optimized('train'),\n",
    "    classes=CONFIG['CLASSES']\n",
    ")\n",
    "\n",
    "val_dataset = BrainMRIDataset(\n",
    "    root_dir=CONFIG['DATA_PATH'] / 'val',\n",
    "    transform=get_transforms_optimized('val'),\n",
    "    classes=CONFIG['CLASSES']\n",
    ")\n",
    "\n",
    "test_dataset = BrainMRIDataset(\n",
    "    root_dir=CONFIG['DATA_PATH'] / 'test',\n",
    "    transform=get_transforms_optimized('test'),\n",
    "    classes=CONFIG['CLASSES']\n",
    ")\n",
    "\n",
    "# DataLoaders optimis√©s\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG['BATCH_SIZE'],\n",
    "    shuffle=True,\n",
    "    num_workers=CONFIG['NUM_WORKERS'],\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True if CONFIG['NUM_WORKERS'] > 0 else False,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=CONFIG['BATCH_SIZE'],\n",
    "    shuffle=False,\n",
    "    num_workers=CONFIG['NUM_WORKERS'],\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True if CONFIG['NUM_WORKERS'] > 0 else False,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=CONFIG['BATCH_SIZE'],\n",
    "    shuffle=False,\n",
    "    num_workers=CONFIG['NUM_WORKERS'],\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True if CONFIG['NUM_WORKERS'] > 0 else False,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Train: {len(train_dataset)} images, {len(train_loader)} batches\")\n",
    "print(f\"‚úÖ Val:   {len(val_dataset)} images, {len(val_loader)} batches\")\n",
    "print(f\"‚úÖ Test:  {len(test_dataset)} images, {len(test_loader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "767825b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèóÔ∏è  CR√âATION DU MOD√àLE...\n",
      "üèóÔ∏è  CR√âATION DU MOD√àLE: RESNET101\n",
      "============================================================\n",
      "‚úÖ Mod√®le charg√©: resnet101\n",
      "   Pretrained: True\n",
      "üî• Backbone d√©gel√© (fine-tuning complet)\n",
      "üìä Architecture de la t√™te:\n",
      "   Input: 2048 features\n",
      "   Hidden: 512 neurons (Dropout 0.5, ReLU, Dropout 0.3)\n",
      "   Output: 4 classes\n",
      "üìà Param√®tres:\n",
      "   Total: 43,551,300\n",
      "   Entra√Ænables: 43,551,300\n",
      "   Gel√©s: 0\n",
      "‚úÖ Mod√®le cr√©√© et d√©plac√© sur: cpu\n",
      "‚öôÔ∏è  CONFIGURATION DE L'OPTIMIZER...\n",
      "‚úÖ Optimizer: AdamW\n",
      "‚úÖ Configuration training termin√©e!\n"
     ]
    }
   ],
   "source": [
    "# CELLULE 5: CR√âATION DU MOD√àLE ET OPTIMIZER\n",
    "print(\"üèóÔ∏è  CR√âATION DU MOD√àLE...\")\n",
    "model = create_model(\n",
    "    model_name=CONFIG['MODEL_NAME'],\n",
    "    num_classes=CONFIG['NUM_CLASSES'],\n",
    "    pretrained=CONFIG['PRETRAINED'],\n",
    "    freeze_backbone=CONFIG['FREEZE_BACKBONE']\n",
    ")\n",
    "model = model.to(device)\n",
    "print(f\"‚úÖ Mod√®le cr√©√© et d√©plac√© sur: {device}\")\n",
    "\n",
    "print(\"‚öôÔ∏è  CONFIGURATION DE L'OPTIMIZER...\")\n",
    "\n",
    "# Mixed Precision Scaler\n",
    "scaler = GradScaler() if torch.cuda.is_available() else None\n",
    "\n",
    "# Optimizer\n",
    "if CONFIG['OPTIMIZER'] == 'AdamW':\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=CONFIG['LEARNING_RATE'],\n",
    "        weight_decay=CONFIG['WEIGHT_DECAY'],\n",
    "        betas=(0.9, 0.999)\n",
    "    )\n",
    "    print(\"‚úÖ Optimizer: AdamW\")\n",
    "else:\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=CONFIG['LEARNING_RATE'],\n",
    "        weight_decay=CONFIG['WEIGHT_DECAY']\n",
    "    )\n",
    "    print(\"‚úÖ Optimizer: Adam\")\n",
    "\n",
    "# Scheduler\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, \n",
    "    T_max=CONFIG['EPOCHS'],\n",
    "    eta_min=1e-6\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"‚úÖ Configuration training termin√©e!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "920de82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fonctions d'entra√Ænement d√©finies!\n"
     ]
    }
   ],
   "source": [
    "# CELLULE 6: FONCTIONS D'ENTRA√éNEMENT\n",
    "def train_epoch_fast(model, loader, criterion, optimizer, device, scaler):\n",
    "    \"\"\"Entra√Ænement avec Mixed Precision\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Training')\n",
    "    for images, labels in pbar:\n",
    "        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "        \n",
    "        # Forward avec Mixed Precision\n",
    "        with autocast(enabled=scaler is not None):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward avec gradient scaling\n",
    "        optimizer.zero_grad()\n",
    "        if scaler:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Statistiques\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # Mise √† jour barre\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{100.*correct/total:.2f}%'\n",
    "        })\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100. * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate_epoch_fast(model, loader, criterion, device):\n",
    "    \"\"\"Validation optimis√©e\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader, desc='Validation')\n",
    "        for images, labels in pbar:\n",
    "            images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            \n",
    "            # Forward avec autocast pour coh√©rence\n",
    "            with autocast(enabled=torch.cuda.is_available()):\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{100.*correct/total:.2f}%'\n",
    "            })\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100. * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc, all_preds, all_labels\n",
    "\n",
    "def print_gpu_usage():\n",
    "    \"\"\"Affiche l'utilisation du GPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"üéÆ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"üíæ M√©moire utilis√©e: {torch.cuda.memory_allocated(0)/1024**3:.2f} GB\")\n",
    "        print(f\"üíæ M√©moire r√©serv√©e: {torch.cuda.memory_reserved(0)/1024**3:.2f} GB\")\n",
    "\n",
    "print(\"‚úÖ Fonctions d'entra√Ænement d√©finies!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8189531a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ D√âBUT DE L'ENTRA√éNEMENT OPTIMIS√â\n",
      "============================================================\n",
      "Mod√®le: resnet101\n",
      "Batch size: 32\n",
      "Image size: (256, 256)\n",
      "Mixed Precision: D√âSACTIV√â (CPU)\n",
      "Device: cpu\n",
      "Train samples: 32391\n",
      "Val samples: 1994\n",
      "============================================================\n",
      "\n",
      "üìÖ Epoch 1/40\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/1013 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# CELLULE 7: BOUCLE D'ENTRA√éNEMENT PRINCIPALE\n",
    "print(\"\\nüöÄ D√âBUT DE L'ENTRA√éNEMENT OPTIMIS√â\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Mod√®le: {CONFIG['MODEL_NAME']}\")\n",
    "print(f\"Batch size: {CONFIG['BATCH_SIZE']}\")\n",
    "print(f\"Image size: {CONFIG['IMAGE_SIZE']}\")\n",
    "print(f\"Mixed Precision: {'ACTIV√â' if torch.cuda.is_available() else 'D√âSACTIV√â (CPU)'}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Val samples: {len(val_dataset)}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Historique\n",
    "history = {\n",
    "    'train_loss': [], 'train_acc': [],\n",
    "    'val_loss': [], 'val_acc': [],\n",
    "    'lr': []\n",
    "}\n",
    "\n",
    "# Early stopping\n",
    "best_val_acc = 0.0\n",
    "patience_counter = 0\n",
    "best_model_path = CONFIG['SAVE_DIR'] / f\"{CONFIG['MODEL_NAME']}_best.pth\"\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(CONFIG['EPOCHS']):\n",
    "    print(f\"\\nüìÖ Epoch {epoch+1}/{CONFIG['EPOCHS']}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Entra√Ænement acc√©l√©r√©\n",
    "    train_loss, train_acc = train_epoch_fast(\n",
    "        model, train_loader, criterion, optimizer, device, scaler\n",
    "    )\n",
    "    \n",
    "    # Validation\n",
    "    val_loss, val_acc, val_preds, val_labels = validate_epoch_fast(\n",
    "        model, val_loader, criterion, device\n",
    "    )\n",
    "    \n",
    "    # Scheduler\n",
    "    scheduler.step()\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Sauvegarder historique\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['lr'].append(current_lr)\n",
    "    \n",
    "    # Affichage\n",
    "    print(f\"üìä Train: {train_acc:.2f}% | Val: {val_acc:.2f}% | LR: {current_lr:.6f}\")\n",
    "    \n",
    "    # Sauvegarde du meilleur mod√®le (bas√© sur accuracy)\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        \n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "            'val_loss': val_loss,\n",
    "            'history': history,\n",
    "            'config': CONFIG\n",
    "        }, best_model_path)\n",
    "        \n",
    "        print(f\"üíæ Meilleur mod√®le sauvegard√©! (val_acc: {val_acc:.2f}%)\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"‚è≥ Early stopping: {patience_counter}/{CONFIG['PATIENCE']}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if CONFIG['EARLY_STOPPING'] and patience_counter >= CONFIG['PATIENCE']:\n",
    "        print(f\"\\nüõë ARR√äT PR√âCOCE ACTIV√â apr√®s {epoch+1} epochs!\")\n",
    "        print(f\"   Meilleur val_acc: {best_val_acc:.2f}%\")\n",
    "        break\n",
    "\n",
    "# Temps total\n",
    "end_time = time.time()\n",
    "total_time = (end_time - start_time) / 60\n",
    "\n",
    "print(f\"\\n‚úÖ ENTRA√éNEMENT TERMIN√â en {total_time:.2f} minutes\")\n",
    "print(f\"üéØ Meilleure val_acc: {best_val_acc:.2f}%\")\n",
    "\n",
    "# Sauvegarde de l'historique\n",
    "history_file = CONFIG['LOG_DIR'] / f\"{CONFIG['MODEL_NAME']}_history.json\"\n",
    "with open(history_file, 'w') as f:\n",
    "    # Convertir les numpy arrays en listes pour JSON\n",
    "    history_serializable = {\n",
    "        'train_loss': [float(x) for x in history['train_loss']],\n",
    "        'train_acc': [float(x) for x in history['train_acc']],\n",
    "        'val_loss': [float(x) for x in history['val_loss']],\n",
    "        'val_acc': [float(x) for x in history['val_acc']],\n",
    "        'lr': [float(x) for x in history['lr']]\n",
    "    }\n",
    "    json.dump(history_serializable, f, indent=2)s\n",
    "print(f\"üíæ Historique sauvegard√©: {history_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35052a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a990fb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AJOUTER ces imports apr√®s les autres\n",
    "from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1528ac7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• Activation du Mixed Precision pour GPU NVIDIA...\n"
     ]
    }
   ],
   "source": [
    "# ‚ö° CONFIGURATION AVEC MIXED PRECISION\n",
    "print(\"üî• Activation du Mixed Precision pour GPU NVIDIA...\")\n",
    "\n",
    "# Mixed Precision Scaler - ESSENTIEL pour la performance\n",
    "scaler = GradScaler() if torch.cuda.is_available() else None\n",
    "\n",
    "# Optimizer am√©lior√©\n",
    "if CONFIG['OPTIMIZER'] == 'AdamW':\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=CONFIG['LEARNING_RATE'],\n",
    "        weight_decay=CONFIG['WEIGHT_DECAY'],\n",
    "        betas=(0.9, 0.999)\n",
    "    )\n",
    "else:\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=CONFIG['LEARNING_RATE'],\n",
    "        weight_decay=CONFIG['WEIGHT_DECAY']\n",
    "    )\n",
    "\n",
    "# Scheduler Cosine - Excellent convergence\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, \n",
    "    T_max=CONFIG['EPOCHS'],\n",
    "    eta_min=1e-6\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3f0bcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_fast(model, loader, criterion, optimizer, device, scaler):\n",
    "    \"\"\"Entra√Ænement avec Mixed Precision\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Training')\n",
    "    for images, labels in pbar:\n",
    "        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "        \n",
    "        # Forward avec Mixed Precision\n",
    "        with autocast(enabled=scaler is not None):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward avec gradient scaling\n",
    "        optimizer.zero_grad()\n",
    "        if scaler:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Statistiques\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # Mise √† jour barre\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{100.*correct/total:.2f}%'\n",
    "        })\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100. * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate_epoch_fast(model, loader, criterion, device):\n",
    "    \"\"\"Validation optimis√©e\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader, desc='Validation')\n",
    "        for images, labels in pbar:\n",
    "            images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            \n",
    "            # Forward avec autocast pour coh√©rence\n",
    "            with autocast(enabled=torch.cuda.is_available()):\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{100.*correct/total:.2f}%'\n",
    "            })\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100. * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b228cd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ CR√âATION DES DATASETS ET DATALOADERS...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'BrainMRIDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     19\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m A.Compose([\n\u001b[32m     20\u001b[39m             A.Resize(CONFIG[\u001b[33m'\u001b[39m\u001b[33mIMAGE_SIZE\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m0\u001b[39m], CONFIG[\u001b[33m'\u001b[39m\u001b[33mIMAGE_SIZE\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m1\u001b[39m]),\n\u001b[32m     21\u001b[39m             A.Normalize(mean=[\u001b[32m0.485\u001b[39m, \u001b[32m0.456\u001b[39m, \u001b[32m0.406\u001b[39m], std=[\u001b[32m0.229\u001b[39m, \u001b[32m0.224\u001b[39m, \u001b[32m0.225\u001b[39m]),\n\u001b[32m     22\u001b[39m             ToTensorV2()\n\u001b[32m     23\u001b[39m         ])\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Cr√©er datasets\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m train_dataset = \u001b[43mBrainMRIDataset\u001b[49m(\n\u001b[32m     27\u001b[39m     root_dir=CONFIG[\u001b[33m'\u001b[39m\u001b[33mDATA_PATH\u001b[39m\u001b[33m'\u001b[39m] / \u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     28\u001b[39m     transform=get_transforms_optimized(\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m     29\u001b[39m     classes=CONFIG[\u001b[33m'\u001b[39m\u001b[33mCLASSES\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     30\u001b[39m )\n\u001b[32m     32\u001b[39m val_dataset = BrainMRIDataset(\n\u001b[32m     33\u001b[39m     root_dir=CONFIG[\u001b[33m'\u001b[39m\u001b[33mDATA_PATH\u001b[39m\u001b[33m'\u001b[39m] / \u001b[33m'\u001b[39m\u001b[33mval\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     34\u001b[39m     transform=get_transforms_optimized(\u001b[33m'\u001b[39m\u001b[33mval\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m     35\u001b[39m     classes=CONFIG[\u001b[33m'\u001b[39m\u001b[33mCLASSES\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     36\u001b[39m )\n\u001b[32m     38\u001b[39m test_dataset = BrainMRIDataset(\n\u001b[32m     39\u001b[39m     root_dir=CONFIG[\u001b[33m'\u001b[39m\u001b[33mDATA_PATH\u001b[39m\u001b[33m'\u001b[39m] / \u001b[33m'\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     40\u001b[39m     transform=get_transforms_optimized(\u001b[33m'\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m     41\u001b[39m     classes=CONFIG[\u001b[33m'\u001b[39m\u001b[33mCLASSES\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     42\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'BrainMRIDataset' is not defined"
     ]
    }
   ],
   "source": [
    "# CELLULE 1: CR√âATION DES DATASETS ET DATALOADERS\n",
    "print(\"üì¶ CR√âATION DES DATASETS ET DATALOADERS...\")\n",
    "\n",
    "# Transformations optimis√©es\n",
    "def get_transforms_optimized(mode='train'):\n",
    "    \"\"\"Pipeline d'augmentation optimis√©\"\"\"\n",
    "    if mode == 'train':\n",
    "        return A.Compose([\n",
    "            A.Resize(CONFIG['IMAGE_SIZE'][0], CONFIG['IMAGE_SIZE'][1]),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.Rotate(limit=10, p=0.3),\n",
    "            A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=10, p=0.3),\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "            A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    else:\n",
    "        return A.Compose([\n",
    "            A.Resize(CONFIG['IMAGE_SIZE'][0], CONFIG['IMAGE_SIZE'][1]),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "# Cr√©er datasets\n",
    "train_dataset = BrainMRIDataset(\n",
    "    root_dir=CONFIG['DATA_PATH'] / 'train',\n",
    "    transform=get_transforms_optimized('train'),\n",
    "    classes=CONFIG['CLASSES']\n",
    ")\n",
    "\n",
    "val_dataset = BrainMRIDataset(\n",
    "    root_dir=CONFIG['DATA_PATH'] / 'val',\n",
    "    transform=get_transforms_optimized('val'),\n",
    "    classes=CONFIG['CLASSES']\n",
    ")\n",
    "\n",
    "test_dataset = BrainMRIDataset(\n",
    "    root_dir=CONFIG['DATA_PATH'] / 'test',\n",
    "    transform=get_transforms_optimized('test'),\n",
    "    classes=CONFIG['CLASSES']\n",
    ")\n",
    "\n",
    "# DataLoaders optimis√©s\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG['BATCH_SIZE'],\n",
    "    shuffle=True,\n",
    "    num_workers=CONFIG['NUM_WORKERS'],\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True if CONFIG['NUM_WORKERS'] > 0 else False,\n",
    "    prefetch_factor=2 if CONFIG['NUM_WORKERS'] > 0 else None,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=CONFIG['BATCH_SIZE'],\n",
    "    shuffle=False,\n",
    "    num_workers=CONFIG['NUM_WORKERS'],\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True if CONFIG['NUM_WORKERS'] > 0 else False,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=CONFIG['BATCH_SIZE'],\n",
    "    shuffle=False,\n",
    "    num_workers=CONFIG['NUM_WORKERS'],\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True if CONFIG['NUM_WORKERS'] > 0 else False,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Train: {len(train_dataset)} images, {len(train_loader)} batches\")\n",
    "print(f\"‚úÖ Val:   {len(val_dataset)} images, {len(val_loader)} batches\")\n",
    "print(f\"‚úÖ Test:  {len(test_dataset)} images, {len(test_loader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50c58e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ D√âBUT DE L'ENTRA√éNEMENT OPTIMIS√â SUR GPU NVIDIA\n",
      "============================================================\n",
      "Mod√®le: resnet101\n",
      "Batch size: 32\n",
      "Image size: (256, 256)\n",
      "Mixed Precision: ACTIV√â\n",
      "Device: cpu\n",
      "============================================================\n",
      "\n",
      "üìÖ Epoch 1/40\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m50\u001b[39m)\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Entra√Ænement acc√©l√©r√©\u001b[39;00m\n\u001b[32m     30\u001b[39m train_loss, train_acc = train_epoch_fast(\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     model, \u001b[43mtrain_loader\u001b[49m, criterion, optimizer, device, scaler\n\u001b[32m     32\u001b[39m )\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Validation\u001b[39;00m\n\u001b[32m     35\u001b[39m val_loss, val_acc, val_preds, val_labels = validate_epoch_fast(\n\u001b[32m     36\u001b[39m     model, val_loader, criterion, device\n\u001b[32m     37\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "# üöÄ BOUCLE D'ENTRA√éNEMENT HAUTE PERFORMANCE\n",
    "print(\"üöÄ D√âBUT DE L'ENTRA√éNEMENT OPTIMIS√â SUR GPU NVIDIA\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Mod√®le: {CONFIG['MODEL_NAME']}\")\n",
    "print(f\"Batch size: {CONFIG['BATCH_SIZE']}\")\n",
    "print(f\"Image size: {CONFIG['IMAGE_SIZE']}\")\n",
    "print(f\"Mixed Precision: ACTIV√â\")\n",
    "print(f\"Device: {device}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Historique\n",
    "history = {\n",
    "    'train_loss': [], 'train_acc': [],\n",
    "    'val_loss': [], 'val_acc': [],\n",
    "    'lr': []\n",
    "}\n",
    "\n",
    "# Early stopping\n",
    "best_val_acc = 0.0\n",
    "patience_counter = 0\n",
    "best_model_path = CONFIG['SAVE_DIR'] / f\"{CONFIG['MODEL_NAME']}_best.pth\"\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(CONFIG['EPOCHS']):\n",
    "    print(f\"\\nüìÖ Epoch {epoch+1}/{CONFIG['EPOCHS']}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Entra√Ænement acc√©l√©r√©\n",
    "    train_loss, train_acc = train_epoch_fast(\n",
    "        model, train_loader, criterion, optimizer, device, scaler\n",
    "    )\n",
    "    \n",
    "    # Validation\n",
    "    val_loss, val_acc, val_preds, val_labels = validate_epoch_fast(\n",
    "        model, val_loader, criterion, device\n",
    "    )\n",
    "    \n",
    "    # Scheduler\n",
    "    scheduler.step()\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Sauvegarder historique\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['lr'].append(current_lr)\n",
    "    \n",
    "    # Affichage\n",
    "    print(f\"üìä Train: {train_acc:.2f}% | Val: {val_acc:.2f}% | LR: {current_lr:.6f}\")\n",
    "    \n",
    "    # Sauvegarde du meilleur mod√®le (bas√© sur accuracy)\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        \n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "            'val_loss': val_loss,\n",
    "            'history': history,\n",
    "            'config': CONFIG\n",
    "        }, best_model_path)\n",
    "        \n",
    "        print(f\"üíæ Meilleur mod√®le sauvegard√©! (val_acc: {val_acc:.2f}%)\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"‚è≥ Early stopping: {patience_counter}/{CONFIG['PATIENCE']}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if CONFIG['EARLY_STOPPING'] and patience_counter >= CONFIG['PATIENCE']:\n",
    "        print(f\"\\nüõë ARR√äT PR√âCOCE ACTIV√â apr√®s {epoch+1} epochs!\")\n",
    "        print(f\"   Meilleur val_acc: {best_val_acc:.2f}%\")\n",
    "        break\n",
    "\n",
    "# Temps total\n",
    "end_time = time.time()\n",
    "total_time = (end_time - start_time) / 60\n",
    "\n",
    "print(f\"\\n‚úÖ ENTRA√éNEMENT TERMIN√â en {total_time:.2f} minutes\")\n",
    "print(f\"üéØ Meilleure val_acc: {best_val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39aef562",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gpu_usage():\n",
    "    \"\"\"Affiche l'utilisation du GPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"üéÆ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"üíæ M√©moire utilis√©e: {torch.cuda.memory_allocated(0)/1024**3:.2f} GB\")\n",
    "        print(f\"üíæ M√©moire r√©serv√©e: {torch.cuda.memory_reserved(0)/1024**3:.2f} GB\")\n",
    "        print(f\"üî• Utilisation: {torch.cuda.utilization(0)}%\")\n",
    "\n",
    "# Appeler avant l'entra√Ænement\n",
    "print_gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb02c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2509b497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des Courbes d'Entra√Ænement\n",
    "def plot_training_history(history):\n",
    "    \"\"\"Visualise les courbes d'entra√Ænement\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    # Loss\n",
    "    axes[0, 0].plot(epochs, history['train_loss'], 'b-o', label='Train Loss', linewidth=2)\n",
    "    axes[0, 0].plot(epochs, history['val_loss'], 'r-o', label='Val Loss', linewidth=2)\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[0, 1].plot(epochs, history['train_acc'], 'b-o', label='Train Acc', linewidth=2)\n",
    "    axes[0, 1].plot(epochs, history['val_acc'], 'r-o', label='Val Acc', linewidth=2)\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy (%)')\n",
    "    axes[0, 1].set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Learning Rate\n",
    "    axes[1, 0].plot(epochs, history['lr'], 'g-o', linewidth=2)\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Learning Rate')\n",
    "    axes[1, 0].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].set_yscale('log')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Overfitting gap\n",
    "    gap = [t - v for t, v in zip(history['train_acc'], history['val_acc'])]\n",
    "    axes[1, 1].plot(epochs, gap, 'm-o', linewidth=2)\n",
    "    axes[1, 1].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Accuracy Gap (%)')\n",
    "    axes[1, 1].set_title('Train-Val Accuracy Gap (Overfitting)', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(CONFIG['FIG_DIR'] / 'training_curves.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistiques\n",
    "    print(\"\\nüìä STATISTIQUES D'ENTRA√éNEMENT:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Meilleure val_acc: {max(history['val_acc']):.2f}% (epoch {history['val_acc'].index(max(history['val_acc']))+1})\")\n",
    "    print(f\"Meilleure val_loss: {min(history['val_loss']):.4f} (epoch {history['val_loss'].index(min(history['val_loss']))+1})\")\n",
    "    print(f\"Gap final train-val: {gap[-1]:.2f}%\")\n",
    "    print(f\"Learning rate final: {history['lr'][-1]:.6f}\")\n",
    "\n",
    "# Afficher les courbes\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a870b95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âvaluation sur le Test Set\n",
    "def evaluate_model(model, loader, device, classes):\n",
    "    \"\"\"√âvalue le mod√®le et retourne toutes les m√©triques\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc='√âvaluation'):\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_probs = np.array(all_probs)\n",
    "    \n",
    "    # M√©triques\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    # Matrice de confusion\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    # Rapport de classification\n",
    "    class_report = classification_report(all_labels, all_preds, target_names=classes, output_dict=True)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'confusion_matrix': cm,\n",
    "        'classification_report': class_report,\n",
    "        'predictions': all_preds,\n",
    "        'labels': all_labels,\n",
    "        'probabilities': all_probs\n",
    "    }\n",
    "\n",
    "# Charger le meilleur mod√®le pour √©valuation\n",
    "print(\"\\nüîç CHARGEMENT DU MEILLEUR MOD√àLE POUR √âVALUATION...\")\n",
    "checkpoint = torch.load(best_model_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"‚úÖ Mod√®le charg√© (epoch {checkpoint['epoch']}, val_loss: {checkpoint['val_loss']:.4f})\")\n",
    "\n",
    "# √âvaluation sur test set\n",
    "print(\"\\nüß™ √âVALUATION SUR LE TEST SET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_metrics = evaluate_model(model, test_loader, device, CONFIG['CLASSES'])\n",
    "\n",
    "# Affichage des r√©sultats\n",
    "print(f\"\\nüìä R√âSULTATS SUR LE TEST SET:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Accuracy:  {test_metrics['accuracy']:.4f}\")\n",
    "print(f\"Precision: {test_metrics['precision']:.4f}\")\n",
    "print(f\"Recall:    {test_metrics['recall']:.4f}\")\n",
    "print(f\"F1-Score:  {test_metrics['f1_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34857643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de Confusion\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(test_metrics['confusion_matrix'], \n",
    "            annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=CONFIG['CLASSES'], \n",
    "            yticklabels=CONFIG['CLASSES'])\n",
    "plt.title('Matrice de Confusion - Test Set', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Pr√©dictions')\n",
    "plt.ylabel('Vraies √©tiquettes')\n",
    "plt.tight_layout()\n",
    "plt.savefig(CONFIG['FIG_DIR'] / 'confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Rapport de classification d√©taill√©\n",
    "print(\"\\nüìã RAPPORT DE CLASSIFICATION D√âTAILL√â:\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(test_metrics['labels'], test_metrics['predictions'], \n",
    "                          target_names=CONFIG['CLASSES']))\n",
    "\n",
    "# Sauvegarde des m√©triques\n",
    "metrics_file = CONFIG['LOG_DIR'] / f\"{CONFIG['MODEL_NAME']}_test_metrics.json\"\n",
    "with open(metrics_file, 'w') as f:\n",
    "    json.dump({k: (float(v) if isinstance(v, (np.floating, float)) else v.tolist() if isinstance(v, np.ndarray) else v) \n",
    "              for k, v in test_metrics.items() if k not in ['predictions', 'labels', 'probabilities']}, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ M√©triques sauvegard√©es: {metrics_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e64ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des Pr√©dictions\n",
    "def visualize_predictions(model, loader, device, classes, num_samples=12):\n",
    "    \"\"\"Visualise des exemples de pr√©dictions\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    images_list = []\n",
    "    preds_list = []\n",
    "    labels_list = []\n",
    "    probs_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            _, preds = outputs.max(1)\n",
    "            \n",
    "            # Stocker pour visualisation\n",
    "            images_list.extend(images.cpu().numpy())\n",
    "            preds_list.extend(preds.cpu().numpy())\n",
    "            labels_list.extend(labels.numpy())\n",
    "            probs_list.extend(probs.cpu().numpy())\n",
    "            \n",
    "            if len(images_list) >= num_samples:\n",
    "                break\n",
    "    \n",
    "    # S√©lectionner un sous-ensemble\n",
    "    indices = np.random.choice(len(images_list), min(num_samples, len(images_list)), replace=False)\n",
    "    \n",
    "    # Cr√©er la figure\n",
    "    fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, ax_idx in enumerate(indices):\n",
    "        image = images_list[ax_idx].transpose(1, 2, 0)\n",
    "        image = (image * [0.229, 0.224, 0.225]) + [0.485, 0.456, 0.406]  # D√©normaliser\n",
    "        image = np.clip(image, 0, 1)\n",
    "        \n",
    "        true_label = labels_list[ax_idx]\n",
    "        pred_label = preds_list[ax_idx]\n",
    "        prob = probs_list[ax_idx][pred_label]\n",
    "        \n",
    "        ax = axes[idx]\n",
    "        ax.imshow(image, cmap='gray')\n",
    "        ax.set_title(f'True: {classes[true_label]}\\nPred: {classes[pred_label]}\\nConf: {prob:.3f}', \n",
    "                    fontsize=10)\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Colorer en vert si correct, rouge si incorrect\n",
    "        color = 'green' if true_label == pred_label else 'red'\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_edgecolor(color)\n",
    "            spine.set_linewidth(3)\n",
    "    \n",
    "    # Cacher les axes vides\n",
    "    for idx in range(len(indices), len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.suptitle('Exemples de Pr√©dictions sur le Test Set\\n(Vert=Correct, Rouge=Incorrect)', \n",
    "                fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(CONFIG['FIG_DIR'] / 'prediction_examples.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Visualiser quelques pr√©dictions\n",
    "print(\"\\nüé® VISUALISATION DES PR√âDICTIONS...\")\n",
    "visualize_predictions(model, test_loader, device, CONFIG['CLASSES'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dc1c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse par Classe\n",
    "def analyze_class_performance(test_metrics, classes):\n",
    "    \"\"\"Analyse d√©taill√©e des performances par classe\"\"\"\n",
    "    \n",
    "    print(\"\\nüìà ANALYSE DES PERFORMANCES PAR CLASSE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Extraire les m√©triques par classe\n",
    "    class_report = test_metrics['classification_report']\n",
    "    cm = test_metrics['confusion_matrix']\n",
    "    \n",
    "    # Cr√©er un DataFrame pour l'analyse\n",
    "    class_stats = []\n",
    "    for i, class_name in enumerate(classes):\n",
    "        stats = class_report[class_name]\n",
    "        class_stats.append({\n",
    "            'Classe': class_name,\n",
    "            'Pr√©cision': f\"{stats['precision']:.3f}\",\n",
    "            'Rappel': f\"{stats['recall']:.3f}\",\n",
    "            'F1-Score': f\"{stats['f1-score']:.3f}\",\n",
    "            'Support': stats['support']\n",
    "        })\n",
    "    \n",
    "    # Afficher le tableau\n",
    "    df_class_stats = pd.DataFrame(class_stats)\n",
    "    print(df_class_stats.to_string(index=False))\n",
    "    \n",
    "    # Visualisation des m√©triques par classe\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Pr√©cision par classe\n",
    "    precision_data = [class_report[cls]['precision'] for cls in classes]\n",
    "    axes[0].bar(classes, precision_data, color='skyblue', edgecolor='black')\n",
    "    axes[0].set_title('Pr√©cision par Classe', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_ylabel('Pr√©cision')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Rappel par classe\n",
    "    recall_data = [class_report[cls]['recall'] for cls in classes]\n",
    "    axes[1].bar(classes, recall_data, color='lightgreen', edgecolor='black')\n",
    "    axes[1].set_title('Rappel par Classe', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_ylabel('Rappel')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # F1-Score par classe\n",
    "    f1_data = [class_report[cls]['f1-score'] for cls in classes]\n",
    "    axes[2].bar(classes, f1_data, color='lightcoral', edgecolor='black')\n",
    "    axes[2].set_title('F1-Score par Classe', fontsize=14, fontweight='bold')\n",
    "    axes[2].set_ylabel('F1-Score')\n",
    "    axes[2].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(CONFIG['FIG_DIR'] / 'class_performance.png', dbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return df_class_stats\n",
    "\n",
    "# Ex√©cuter l'analyse\n",
    "class_performance = analyze_class_performance(test_metrics, CONFIG['CLASSES'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e99d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation sur l'Ensemble de Validation\n",
    "print(\"\\nüîç VALIDATION SUR L'ENSEMBLE DE VALIDATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# √âvaluation sur validation set\n",
    "val_metrics = evaluate_model(model, val_loader, device, CONFIG['CLASSES'])\n",
    "\n",
    "print(f\"\\nüìä R√âSULTATS SUR VALIDATION SET:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Accuracy:  {val_metrics['accuracy']:.4f}\")\n",
    "print(f\"Precision: {val_metrics['precision']:.4f}\")\n",
    "print(f\"Recall:    {val_metrics['recall']:.4f}\")\n",
    "print(f\"F1-Score:  {val_metrics['f1_score']:.4f}\")\n",
    "\n",
    "# Comparaison Test vs Validation\n",
    "print(\"\\nüìà COMPARAISON TEST vs VALIDATION\")\n",
    "print(\"=\" * 60)\n",
    "comparison_data = {\n",
    "    'Dataset': ['Test', 'Validation'],\n",
    "    'Accuracy': [test_metrics['accuracy'], val_metrics['accuracy']],\n",
    "    'Precision': [test_metrics['precision'], val_metrics['precision']],\n",
    "    'Recall': [test_metrics['recall'], val_metrics['recall']],\n",
    "    'F1-Score': [test_metrics['f1_score'], val_metrics['f1_score']]\n",
    "}\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "print(df_comparison.to_string(index=False))\n",
    "\n",
    "# Visualisation de la comparaison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(comparison_data['Dataset']))\n",
    "width = 0.2\n",
    "\n",
    "metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "colors = ['skyblue', 'lightgreen', 'lightcoral', 'gold']\n",
    "\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    ax.bar(x + i*width, comparison_data[metric], width, label=metric, color=colors[i], edgecolor='black')\n",
    "\n",
    "ax.set_xlabel('Dataset')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Comparaison des Performances: Test vs Validation', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x + 1.5*width)\n",
    "ax.set_xticklabels(comparison_data['Dataset'])\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(CONFIG['FIG_DIR'] / 'test_val_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c6432b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©sum√© Final et Export\n",
    "def create_training_summary(history, test_metrics, val_metrics, config, training_time):\n",
    "    \"\"\"Cr√©e un r√©sum√© complet de l'entra√Ænement\"\"\"\n",
    "    \n",
    "    summary = {\n",
    "        'model_name': config['MODEL_NAME'],\n",
    "        'training_time_minutes': training_time / 60,\n",
    "        'total_epochs_trained': len(history['train_loss']),\n",
    "        'best_epoch': history['val_loss'].index(min(history['val_loss'])) + 1,\n",
    "        'best_val_loss': min(history['val_loss']),\n",
    "        'best_val_acc': max(history['val_acc']),\n",
    "        'final_train_acc': history['train_acc'][-1],\n",
    "        'final_val_acc': history['val_acc'][-1],\n",
    "        'test_metrics': {\n",
    "            'accuracy': float(test_metrics['accuracy']),\n",
    "            'precision': float(test_metrics['precision']),\n",
    "            'recall': float(test_metrics['recall']),\n",
    "            'f1_score': float(test_metrics['f1_score'])\n",
    "        },\n",
    "        'val_metrics': {\n",
    "            'accuracy': float(val_metrics['accuracy']),\n",
    "            'precision': float(val_metrics['precision']),\n",
    "            'recall': float(val_metrics['recall']),\n",
    "            'f1_score': float(val_metrics['f1_score'])\n",
    "        },\n",
    "        'overfitting_gap': float(history['train_acc'][-1] - history['val_acc'][-1]),\n",
    "        'config': {k: v for k, v in config.items() if k not in ['SAVE_DIR', 'LOG_DIR', 'FIG_DIR', 'DATA_PATH']},\n",
    "        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Cr√©er le r√©sum√©\n",
    "summary = create_training_summary(history, test_metrics, val_metrics, CONFIG, total_time)\n",
    "\n",
    "# Sauvegarder le r√©sum√©\n",
    "summary_file = CONFIG['LOG_DIR'] / f\"{CONFIG['MODEL_NAME']}_summary.json\"\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(summary, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\nüìÑ R√âSUM√â COMPLET SAUVEGARD√â: {summary_file}\")\n",
    "\n",
    "# Affichage final\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üéâ ENTRA√éNEMENT ET √âVALUATION TERMIN√âS AVEC SUCC√àS!\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"üìä Performance finale:\")\n",
    "print(f\"   ‚úÖ Test Accuracy:  {test_metrics['accuracy']:.4f}\")\n",
    "print(f\"   ‚úÖ Test F1-Score:  {test_metrics['f1_score']:.4f}\")\n",
    "print(f\"   ‚úÖ Val Accuracy:   {val_metrics['accuracy']:.4f}\")\n",
    "print(f\"   ‚úÖ Val F1-Score:   {val_metrics['f1_score']:.4f}\")\n",
    "print(f\"‚è±Ô∏è  Temps total: {total_time/60:.2f} minutes\")\n",
    "print(f\"üìÅ Mod√®le sauvegard√©: {best_model_path}\")\n",
    "print(f\"üìà Visualisations: {CONFIG['FIG_DIR']}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5ce4af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
